{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zizou.data import DataSource, GeoMagWaveforms\n",
    "from zizou.ssam import SSAM \n",
    "from datetime import datetime, date, timedelta, timezone\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tonik import Storage\n",
    "from io import StringIO\n",
    "from obspy import UTCDateTime, Trace\n",
    "import numpy as np\n",
    "from zizou.visualise import plot_ssam_plotly\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from zizou.dsar import DSAR\n",
    "from zizou.rsam import RSAM, EnergyExplainedByRSAM\n",
    "from zizou.ssam import SSAM\n",
    "from zizou.spectral_features import SpectralFeatures\n",
    "from zizou.data import DataSource, MockSDSWaveforms\n",
    "from obspy import UTCDateTime\n",
    "\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gmc = GeoMagWaveforms(base_url='https://tilde.geonet.org.nz/v3/data',\n",
    "#                       method='60s', aspect='F-total-field', name='magnetic-field')\n",
    "gmc = GeoMagWaveforms(base_url='https://tilde.geonet.org.nz/v3/data',\n",
    "                      method='60s', aspect='H-horizontal-intensity', name='magnetic-field')\n",
    "ssam = SSAM(per_lap=0.5, timestamp='start', interval=1024*60.,\n",
    "            frequencies=np.linspace(1/1000., 1/120., 50))\n",
    "\n",
    "rsam = RSAM(filtertype=\"bp\", filterfreq=(0.001, 0.1), interval=1024*60.)\n",
    "spec = SpectralFeatures(filtertype=\"bp\", filterfreq=(0.001, 0.1), interval=1024*60.)\n",
    "dsar = DSAR(lowerfreqband=(0.0001, 0.001), higherfreqband=(0.001, 0.008), interval=1024*60.)\n",
    "#dsar = DSAR(lowerfreqband=(0.01, 1), higherfreqband=(1, 2.5), interval=1024*60.)\n",
    "rsam_energy_prop = EnergyExplainedByRSAM(filterfreq_wb=(0.001,0.1))\n",
    "\n",
    "ds = DataSource(clients=[gmc])\n",
    "sg = Storage('EYR', rootdir='/tmp/geomag/features')\n",
    "st = sg.get_substore('EYWM', '50', 'H')\n",
    "\n",
    "\n",
    "startdate = UTCDateTime(2024, 5, 6)\n",
    "#enddate = UTCDateTime.now()\n",
    "#enddate = UTCDateTime(2024, 5, 12)\n",
    "enddate = UTCDateTime(2024, 10, 20)\n",
    "\n",
    "\n",
    "\n",
    "for tr in ds.get_waveforms(net='NZ', site='EYWM', loc='50', comp='H',\n",
    "                           start=startdate, end=enddate,\n",
    "                           cache=True):\n",
    "    print(tr)\n",
    "    # for feat in (ssam, rsam, spec):\n",
    "    #     st.save(feat.compute(trace=tr))\n",
    "    \n",
    "    for feat in (dsar, spec, ssam, rsam, rsam_energy_prop):\n",
    "        st.save(feat.compute(trace=tr))\n",
    "\n",
    "\n",
    "    # xds = ssam.compute(tr) \n",
    "\n",
    "    # xds = ssam.compute(tr) \n",
    "    # st.save(xds)\n",
    "\n",
    "    # for tr in ds.get_waveforms(net=\"NZ\", site=site, loc=sensor, comp=channel, start=UTCDateTime(\"2024-05-01\"), end=UTCDateTime(\"2024-06-01\")):\n",
    "    #     print(tr)\n",
    "    #     for feat in (rsam, rsam_energy_prop, ssam, dsar, spec):\n",
    "    #         store.save(feat.compute(trace=tr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.starttime = startdate.datetime\n",
    "st.endtime = enddate.datetime\n",
    "ssam = st('ssam')\n",
    "rsam = st('rsam')\n",
    "filtbank = st('filterbank')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtbank.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 5), sharex=True)\n",
    "ax1.plot(rsam.coords[\"datetime\"], rsam.data)\n",
    "ax1.set_ylabel(\"RSAM\")\n",
    "ax2.imshow(ssam.data, extent=[rsam.coords[\"datetime\"][0].values, rsam.coords[\"datetime\"][-1].values, 0, 5])\n",
    "ax2.set_aspect(\"auto\")\n",
    "ax2.set_ylabel(\"SSAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=2, cols=1, shared_xaxes=True)\n",
    "tr = next(ds.get_waveforms(net='NZ', site='EYWM', loc='50', comp='F',\n",
    "                           start=startdate, end=enddate))\n",
    "dt = [startdate.datetime + timedelta(seconds=s) for s in tr.times()]\n",
    "fig.add_trace(go.Scatter(x=dt, y=tr.data, mode='lines'), row=1, col=1)\n",
    "fig.add_trace(plot_ssam_plotly(filtbank, dbscale=True, new_fig=False), row=2, col=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of autoencoder and clustuirng (using built in autoencoder in Zizou)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from obspy import UTCDateTime\n",
    "import numpy as np\n",
    "\n",
    "from zizou.autoencoder import AutoEncoder\n",
    "\n",
    "#model configuration\n",
    "config = \"\"\"\n",
    "autoencoder: \n",
    "    layers: [2000,500,200,6]\n",
    "    epochs: 5\n",
    "    patience: 5\n",
    "\"\"\"\n",
    "\n",
    "#feature request: \n",
    "sg = Storage('EYR', rootdir='/tmp/geomag/features')\n",
    "st = sg.get_substore('EYWM', '50', 'H')\n",
    "st.station = 'EYWM.50.H'\n",
    "st.starttime = startdate.datetime\n",
    "st.endtime = enddate.datetime\n",
    "model = AutoEncoder(st, configfile=config, batch_size=32, features=['filterbank'])\n",
    "model.clear()\n",
    "# -- Train the Auto-Encoder and save the model (this might take a while)\n",
    "classifications = model.fit_transform(startdate.datetime, enddate.datetime)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.starttime = startdate.datetime\n",
    "st.endtime = enddate.datetime\n",
    "ac_loss = st('autoencoder_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=3, cols=1, shared_xaxes=True)\n",
    "tr = next(ds.get_waveforms(net='NZ', site='EYWM', loc='50', comp='F',\n",
    "                           start=startdate, end=enddate))\n",
    "dt = [startdate.datetime + timedelta(seconds=s) for s in tr.times()]\n",
    "fig.add_trace(go.Scatter(x=pd.to_datetime(ac_loss.datetime), y=np.log10(ac_loss.data), mode='lines'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=dt, y=tr.data, mode='lines'), row=2, col=1)\n",
    "fig.add_trace(plot_ssam_plotly(filtbank, dbscale=True, new_fig=False), row=3, col=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing developing LSTM Autoencoder and clusturing => new version \n",
    "Autoencoders for anomaly detection in time series data. For clustering and anomaly detection in time series data, you can combine an LSTM Autoencoder for feature extraction and K-means or other clustering algorithms for identifying anomalous patterns.\n",
    "\n",
    "Steps of LSTM Autoencoder for time series anomaly detection and clustering:\n",
    "\n",
    "    Data preparation (sliding window method for time series).\n",
    "    LSTM Autoencoder Model (for feature extraction).\n",
    "    Train the autoencoder and use reconstruction error for anomaly detection.\n",
    "    Clustering the latent features using K-means or other clustering methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data preperation\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# time series data\n",
    "tr = next(ds.get_waveforms(net='NZ', site='EYWM', loc='50', comp='H', start=startdate, end=enddate))\n",
    "dt = [startdate.datetime + timedelta(seconds=s) for s in tr.times()]\n",
    "time_series1 = tr.data\n",
    "time_series2 = ssam.data\n",
    "time_series3 = filtbank.data\n",
    "\n",
    "# # Stack the time series into a 2D array where each column is a feature\n",
    "# time_series_multi = np.vstack((time_series1, time_series2)).T  # Shape: (1000, 2)\n",
    "\n",
    "# # Normalize each feature independently\n",
    "# scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "# time_series_scaled = scaler.fit_transform(time_series_multi)\n",
    "\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "time_series_scaled = scaler.fit_transform(time_series3.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "#Create sliding windows for time series forecasting\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        seq = data[i:i + seq_length]\n",
    "        sequences.append(seq)\n",
    "    return np.array(sequences)\n",
    "\n",
    "seq_length = 120\n",
    "X = create_sequences(time_series_scaled, seq_length)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X, dtype=torch.float32).unsqueeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. LSTM Autoencoder model\n",
    "class LSTM_Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTM_Autoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder_lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_lstm = nn.LSTM(hidden_size, input_size, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        _, (hidden, _) = self.encoder_lstm(x)  # Only hidden state is used\n",
    "        hidden = hidden[-1].unsqueeze(0).repeat(x.size(1), 1, 1)  # Repeat hidden state\n",
    "        \n",
    "        # Decoder\n",
    "        decoded, _ = self.decoder_lstm(hidden)\n",
    "        return decoded\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 1\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "\n",
    "model = LSTM_Autoencoder(input_size, hidden_size, num_layers)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the autoencoder: \n",
    "num_epochs = 10\n",
    "model.train()\n",
    "\n",
    "# In your training loop, make sure the input is [batch_size, seq_len, input_size]\n",
    "# If necessary, transpose the input and target before feeding into the model\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Ensure input shape is [batch_size, seq_len, input_size]\n",
    "    inputs = X_train # Transpose batch and sequence dimensions if needed\n",
    "    \n",
    "    outputs = model(inputs)  # Model forward pass\n",
    "    \n",
    "    # Ensure the target is also in the right shape\n",
    "    targets = X_train.transpose(0, 1)  # Targets should match input shape\n",
    "    \n",
    "    loss = criterion(outputs, targets)  # Calculate loss\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained, we can use the reconstruction error (the difference between the input and the reconstructed output) to detect anomalies. High reconstruction error indicates an anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#anomaly detection: \n",
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Reconstruct the time series or ssam\n",
    "with torch.no_grad():\n",
    "    reconstructed = model(X_train).cpu().numpy()\n",
    "\n",
    "reconstructed_transposed = np.transpose(reconstructed, (1, 0, 2))\n",
    "# Calculate the reconstruction error\n",
    "reconstruction_errors = np.mean(np.abs(reconstructed_transposed - X_train.cpu().numpy()), axis=1)\n",
    "\n",
    "# Set an anomaly detection threshold based on the reconstruction error\n",
    "threshold = np.percentile(reconstruction_errors, 95)  # 95th percentile of reconstruction errors\n",
    "\n",
    "# Detect anomalies\n",
    "anomalies = reconstruction_errors > threshold\n",
    "\n",
    "# Plot the reconstruction errors and anomalies\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(reconstruction_errors, label='Reconstruction Errors')\n",
    "plt.axhline(y=threshold, color='r', linestyle='--', label='Threshold')\n",
    "plt.scatter(np.where(anomalies)[0], reconstruction_errors[anomalies], color='red', label='Anomalies')\n",
    "plt.title('Reconstruction Errors and Detected Anomalies')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume `X_test` is the original input data and `reconstructed_test` is the model's output\n",
    "# Convert tensors to numpy arrays\n",
    "X_train_np = X_train.cpu().numpy()\n",
    "reconstructed_train_np = reconstructed_transposed\n",
    "\n",
    "# Plot a sample (e.g., the first time series from the test set)\n",
    "sample_idx = 8000  # Choose a sample index to visualize\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(X_train_np[sample_idx], label='Original', linestyle='-', marker='o')\n",
    "plt.plot(reconstructed_train_np[sample_idx], label='Reconstructed', linestyle='--', marker='x')\n",
    "plt.title(f'Original vs Reconstructed for Sample {sample_idx}')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM autoencoder's hidden states can serve as feature vectors for clustering. You can apply K-means clustering on these latent features to group similar time series patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5: Cluster Latent Features\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Extract hidden states from the encoder as features for clustering\n",
    "def get_hidden_features(model, X):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, (hidden, _) = model.encoder_lstm(X)\n",
    "        features = hidden[-1].cpu().numpy()\n",
    "    return features\n",
    "\n",
    "# Get latent features from the encoder\n",
    "latent_features = get_hidden_features(model, X_train)\n",
    "\n",
    "# Apply K-means clustering\n",
    "n_clusters = 2\n",
    "kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(latent_features)\n",
    "\n",
    "# Plot clusters\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(np.arange(len(cluster_labels)), cluster_labels, c=cluster_labels, cmap='viridis', marker='o')\n",
    "plt.title('K-means Clustering of Latent Features')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Cluster Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sapi's testing 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=2, cols=1, shared_xaxes=True)\n",
    "tr = next(ds.get_waveforms(net='NZ', site='EYWM', loc='50', comp='H',\n",
    "                           start=startdate, end=enddate))\n",
    "dt = [startdate.datetime + timedelta(seconds=s) for s in tr.times()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = tr.data\n",
    "time_steps = 3600 # Length of each segment\n",
    "step = 1         # Step size (1 means fully overlapping segments)\n",
    "n_features = 1   # Number of features (e.g., one magnetogram channel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_segments(data, time_steps, step, n_features):\n",
    "    X, y = [], []\n",
    "    for i in range(0, len(data) - time_steps, step):\n",
    "        segment = data[i:i + time_steps]\n",
    "        X.append(segment)\n",
    "        y.append(data[i + time_steps])  # Predicting the next value as the target\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    X = X.reshape((X.shape[0], X.shape[1], n_features))  # Reshape to [samples, time_steps, n_features]\n",
    "    return X, y\n",
    "\n",
    "X, y = create_segments(time_series, time_steps, step, n_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "# Split into training and testing sets - 80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check shapes\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "# X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "# y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "# X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "# y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the LSTIM model in pytorch => need to test this\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Modified LSTM model to return hidden states as feature vectors\n",
    "class LSTMFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTMFeatureExtractor, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        return hn[-1]  # Use the last hidden state as the feature vector\n",
    "\n",
    "        \n",
    "\n",
    "# Define model parameters\n",
    "input_size = n_features  # Number of features\n",
    "hidden_size = 10      # Number of hidden units in each LSTM layer\n",
    "num_layers = 5     # Number of LSTM layers\n",
    "#output_size = 1         # Size of the feature vector\n",
    "\n",
    "# Initialize the model\n",
    "model = LSTMFeatureExtractor(input_size, hidden_size, num_layers)\n",
    "\n",
    "model = LSTMFeatureExtractor(input_size, hidden_size, num_layers)\n",
    "model.eval()\n",
    "\n",
    "# Extract feature vectors from the LSTM model\n",
    "with torch.no_grad():\n",
    "    feature_vectors = model(X_train).cpu().numpy()  # Feature vectors of shape (batch_size, hidden_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clusturing feature vectors\n",
    "\n",
    "# Number of clusters you want to create\n",
    "n_clusters = 2\n",
    "\n",
    "# Apply K-means clustering\n",
    "kmeans = KMeans(n_clusters=n_clusters, n_init=20, random_state=42)\n",
    "clusters = kmeans.fit_predict(feature_vectors)\n",
    "\n",
    "# Plot the clusters for visualization (only if the feature dimension is 2 or 3)\n",
    "plt.scatter(feature_vectors[:, 0], feature_vectors[:, 1], c=clusters, cmap='viridis')\n",
    "plt.title('Clusters of LSTM Feature Vectors')\n",
    "plt.xlabel('Feature Dimension 1')\n",
    "plt.ylabel('Feature Dimension 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'clusters' contains the cluster labels for each segment\n",
    "# 'X_train' contains the original segments of the time series\n",
    "\n",
    "# Example: Assigning cluster labels to each segment\n",
    "labeled_segments = list(zip(X_train, clusters))\n",
    "\n",
    "# Example function to map clusters back to the original waveform\n",
    "def map_clusters_to_waveform(time_series, time_steps, step, clusters):\n",
    "    cluster_mapping = np.full(len(time_series), np.nan)  # Initialize with NaNs\n",
    "    segment_start = 0\n",
    "    \n",
    "    for i, cluster_label in enumerate(clusters):\n",
    "        segment_end = segment_start + time_steps\n",
    "        cluster_mapping[segment_start:segment_end] = cluster_label\n",
    "        segment_start += step  # Move by the step size\n",
    "    \n",
    "    return cluster_mapping\n",
    "\n",
    "# Assuming you have the original time series 'time_series' and cluster labels 'clusters'\n",
    "cluster_mapping = map_clusters_to_waveform(time_series, time_steps, 1, clusters)\n",
    "\n",
    "# Plot the original waveform with cluster labels\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(time_series, label='Original Waveform')\n",
    "plt.scatter(np.arange(len(time_series)), cluster_mapping * max(time_series), color='red', label='Cluster Labels', marker='o')\n",
    "plt.title('Waveform with Clustered Events')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "# Training\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 20\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    train_loss = criterion(outputs, y_train)\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Store training loss\n",
    "    train_losses.append(train_loss.item())\n",
    "    \n",
    "    # Validation/Test loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test)\n",
    "        test_loss = criterion(test_outputs, y_test)\n",
    "        test_losses.append(test_loss.item())\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss.item():.4f}, Test Loss: {test_loss.item():.4f}')\n",
    "\n",
    "# # Plotting\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(train_losses, label='Train Loss')\n",
    "# plt.plot(test_losses, label='Test Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Model Loss Progression')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model testing\n",
    "# Testing\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test)\n",
    "    test_loss = criterion(test_outputs, y_test)\n",
    "\n",
    "print(f'Test Loss: {test_loss.item():.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
